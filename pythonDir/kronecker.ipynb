{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kronecker-based Compressed Sensing signal recovery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we explore the _compression_ and _recovery_ of aCompressive Sensing _compressible_ signal through __Compressive Sensing__ with __Kronecker Technique__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def printFormatted(matrix, decimals=4):\n",
    "    \"\"\"\n",
    "    Prints the matrix with formatted elements aligned in columns for improved readability.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    matrix : numpy array\n",
    "        The matrix to be printed.\n",
    "    decimals : int, optional (default=4)\n",
    "        The number of decimal places for formatting the elements.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    None\n",
    "        This function does not return any value; it prints the formatted matrix directly to the console.\n",
    "\n",
    "    Notes:\n",
    "    -----\n",
    "    - The function aligns columns based on the maximum width needed for the formatted elements, ensuring the matrix is displayed neatly.\n",
    "    - This function is useful for visual inspection of numerical matrices, especially those with varying magnitudes.\n",
    "    \n",
    "    Example:\n",
    "    --------\n",
    "    >>> import numpy as np\n",
    "    >>> matrix = np.array([[1.234567, 123.456789], [0.0001234, 1.2345]])\n",
    "    >>> print('Classic print:')\n",
    "    >>> print(matrix)\n",
    "    Classic print:\n",
    "    [[1.2345670e+00 1.2345679e+02]\n",
    "     [1.2340000e-04 1.2345000e+00]]\n",
    "     \n",
    "    >>> print('\\nFormatted print:')\n",
    "    >>> printFormatted(matrix, decimals=4)\n",
    "         1.2346  123.4568\n",
    "         0.0001    1.2345\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # Determine the maximum width needed to keep alignment\n",
    "    max_width = max(len(f'{value:.{decimals}f}') for row in matrix for value in row)\n",
    "\n",
    "    # Create a formatted string for each element in the matrix, ensuring alignment\n",
    "    formatted_matrix = '\\n'.join([' '.join([f'{value:>{max_width}.{decimals}f}' for value in row]) for row in matrix])\n",
    "\n",
    "    # Print the formatted matrix\n",
    "    print(formatted_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a random sparse signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sparseSignal(N, K=None, sigma_inactive=0.01, sigma_active=0.5, fixedActiveValue=None):\n",
    "    \"\"\"\n",
    "    Generates a K-sparse signal with N-K inactive components, chosen randomly. The inactive components \n",
    "    are nearly zero, while the active components are either a fixed value or generated from a Gaussian\n",
    "    distribution.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    N : int\n",
    "        The total number of components in the signal.\n",
    "    \n",
    "    K : int, optional\n",
    "        The number of active (non-zero) components in the signal. If not provided, defaults to 10% of `N`.\n",
    "    \n",
    "    sigma_inactive : float, optional (default=0.01)\n",
    "        The standard deviation of the Gaussian noise added to the inactive components.\n",
    "    \n",
    "    sigma_active : float, optional (default=0.5)\n",
    "        The standard deviation of the Gaussian noise for generating the active components, if `fixedActiveValue` \n",
    "        is not provided.\n",
    "    \n",
    "    fixedActiveValue : float, optional\n",
    "        If provided, this fixed value is assigned to all active components instead of generating them randomly.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    signal : numpy array\n",
    "        The generated signal of length `N` with `K` active components.\n",
    "    \n",
    "    active_indices : numpy array\n",
    "        The indices of the active components in the signal.\n",
    "\n",
    "    Notes:\n",
    "    -----\n",
    "    - The signal is constructed by first randomly selecting `K` indices as active components.\n",
    "    - If `fixedActiveValue` is `None`, the active components are drawn from a Gaussian distribution \n",
    "      with standard deviation `sigma_active`.\n",
    "    - Gaussian noise with standard deviation `sigma_inactive` is then added to the inactive components, \n",
    "      ensuring they have small random values near zero.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if K is None:\n",
    "        N = int(N)\n",
    "        K = int(0.1 * N)\n",
    "    else:\n",
    "        N = int(N)\n",
    "        K = int(K)\n",
    "\n",
    "    active_indexes = np.zeros(N)\n",
    "    active_indexes[:K] = 1\n",
    "    np.random.shuffle(active_indexes)\n",
    "\n",
    "    signal = np.zeros(N)\n",
    "    \n",
    "    if fixedActiveValue is None:\n",
    "        # Generate active components with Gaussian noise\n",
    "        signal[active_indexes == 1] = np.random.randn(K) * sigma_active\n",
    "    else:\n",
    "        # Use fixed value for active components\n",
    "        signal[active_indexes == 1] = fixedActiveValue\n",
    "    \n",
    "    # Add Gaussian noise only to inactive components\n",
    "    signal[active_indexes == 0] += np.random.randn(N - K) * sigma_inactive\n",
    "\n",
    "    return signal, np.where(active_indexes == 1)[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the above function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Parameters for the sparse signal\n",
    "N = 2 ** 10\n",
    "K = int(0.1 * N)\n",
    "sigma_noise = 0.01\n",
    "sigma_active = 0.2\n",
    "fixedActiveValue = None\n",
    "\n",
    "# Generate the sparse signal\n",
    "x, active_indices = sparseSignal(N, K, sigma_noise, sigma_active, fixedActiveValue)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Measurement matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def generate_DBDD_matrix(M, N):\n",
    "    \"\"\"\n",
    "    Generates a deterministic Diagonally Blocked Block Diagonal (DBBD) matrix.\n",
    "\n",
    "    Requires:\n",
    "    ----------\n",
    "    numpy as np\n",
    "\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    M : int\n",
    "        Number of rows in the matrix.\n",
    "    N: int\n",
    "        Number of columns in the matrix. Should be a multiple of M.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    A : numpy array\n",
    "        The generated DBBD matrix of shape (M, N).\n",
    "    \"\"\"\n",
    "    \n",
    "    if N % M != 0:\n",
    "        raise ValueError(\"N should be a multiple of M.\")\n",
    "    \n",
    "    Phi = np.zeros((M, N))\n",
    "    m = N // M\n",
    "    \n",
    "    for i in range(M):\n",
    "        Phi[i, i*m:(i+1)*m] = 1\n",
    "\n",
    "    return Phi\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def generate_random_matrix(M, N, matrix_type='gaussian'):\n",
    "    \"\"\"\n",
    "    Generates a random matrix based on the specified type.\n",
    "\n",
    "    Requires:\n",
    "    ----------\n",
    "    numpy as np\n",
    "    \n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    M : int\n",
    "        Number of rows in the matrix.\n",
    "    N : int\n",
    "        Number of columns in the matrix.\n",
    "    matrix_type : str, optional (default='gaussian')\n",
    "        The type of random matrix to generate. Options are:\n",
    "        - 'gaussian': A matrix with entries drawn from a normal distribution.\n",
    "        - 'scaled_binary': A matrix with binary entries (±0.5), scaled by 1/sqrt(M).\n",
    "        - 'unscaled_binary': A matrix with binary entries (±1), with no scaling.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    A : numpy array\n",
    "        The generated random matrix of shape (M, N).\n",
    "    \"\"\"\n",
    "    \n",
    "    if matrix_type == 'gaussian':\n",
    "        A = ((1/M)**2) * np.random.randn(M, N)\n",
    "\n",
    "    elif matrix_type == 'scaled_binary':\n",
    "        A = np.random.binomial(1, 0.5, size=(M, N)) - 0.5\n",
    "        A = (1/np.sqrt(M)) * A\n",
    "\n",
    "    elif matrix_type == 'unscaled_binary':\n",
    "        A = np.random.binomial(1, 0.5, size=(M, N)) * 2 - 1\n",
    "        # This generates -1 or +1 directly without scaling\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported matrix type. Choose either 'gaussian', 'scaled_binary', or 'unscaled_binary'.\")\n",
    "\n",
    "    return A\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test above code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 2 ** 4\n",
    "M = N // 4\n",
    "\n",
    "# Generate a deterministic DBBD matrix\n",
    "A = generate_DBDD_matrix(M, N)\n",
    "print(f'DBBD:\\n{A}\\n\\n')\n",
    "\n",
    "# Generate a random Gaussian matrix\n",
    "B = generate_random_matrix(M, N, matrix_type='gaussian')\n",
    "print(f'gaussian:\\n{B}\\n\\n')\n",
    "\n",
    "# Generate a random scaled binary matrix\n",
    "C = generate_random_matrix(M, N, matrix_type='scaled_binary')\n",
    "print(f'scaled binary:\\n{C}\\n\\n')\n",
    "\n",
    "# Generate a random unscaled binary matrix\n",
    "D = generate_random_matrix(M, N, matrix_type='unscaled_binary')\n",
    "print(f'unscaled binary:\\n{D}\\n\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DCT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.fftpack as fftpack\n",
    "\n",
    "def generate_dct_dictionary(N):\n",
    "    \"\"\"\n",
    "    Generates a Discrete Cosine Transform (DCT) basis dictionary.\n",
    "\n",
    "    Requires:\n",
    "    ----------\n",
    "    numpy as np\n",
    "    scipy.fftpack as fftpack\n",
    "\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    N : int\n",
    "        The size of the dictionary (i.e., the length of the signal).\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    dict_matrix : numpy array\n",
    "        The generated DCT dictionary matrix of shape (N, N).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate a DCT basis dictionary\n",
    "    dict_matrix = fftpack.dct(np.eye(N), norm='ortho')\n",
    "    \n",
    "    return dict_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ANY wavelet__ dictionary, choose which __wavelet__ and __level__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pywt\n",
    "\n",
    "def generate_dwt_basis(dim, wavelet='db5', mode='per', level=None):\n",
    "    \"\"\"\n",
    "    Generates a wavelet orthonormal basis matrix using the Discrete Wavelet Transform (DWT).\n",
    "\n",
    "    Requires:\n",
    "    --------\n",
    "    - numpy (as np)\n",
    "    - pywt (PyWavelets)\n",
    "    \n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dim : int\n",
    "        The dimension of the basis (i.e., the length of the signal and the size of the matrix).\n",
    "    \n",
    "    wavelet : str, optional (default='db5')\n",
    "        The name of the wavelet to use. Options include Daubechies wavelets ('db1' to 'db20') and\n",
    "        other wavelet families available in PyWavelets.\n",
    "\n",
    "    mode : str, optional (default='per')\n",
    "        The signal extension mode to use when applying the wavelet transform. The default is\n",
    "        'per' for periodic extension. Other options include 'zero', 'symmetric', etc.\n",
    "\n",
    "    level : int, optional\n",
    "        The level of decomposition to perform. If not provided (level=None), the function sets\n",
    "        the level to log2(dim). If a value is provided, it should be an integer greater than 0.\n",
    "        If the provided level is greater than log2(dim), the function resets it to log2(dim).\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    basis_matrix : numpy array\n",
    "        The generated wavelet basis matrix of shape (dim, dim). Each column of this matrix \n",
    "        represents a wavelet basis vector for the given wavelet and decomposition level.\n",
    "\n",
    "    Raises:\n",
    "    -------\n",
    "    ValueError\n",
    "        If `dim` is not a power of 2, or if `level` is not an integer greater than 0.\n",
    "\n",
    "    Notes:\n",
    "    -----\n",
    "    - This function assumes that `dim` is a power of 2. If it is not, the function raises a ValueError.\n",
    "    - If the decomposition level is not provided, it defaults to log2(dim), ensuring a full decomposition.\n",
    "    - The function constructs the basis by applying the wavelet transform to each standard basis vector\n",
    "      (i.e., the columns of the identity matrix) and concatenating the resulting coefficients into the\n",
    "      final basis matrix.\n",
    "\n",
    "    Changes Made:\n",
    "    -------------\n",
    "    - Added validation for `dim` to ensure it is a power of 2.\n",
    "    - Added checks for the `level` parameter to ensure it is a positive integer and does not exceed log2(dim).\n",
    "    - The function now defaults to log2(dim) if the level is not provided or is greater than log2(dim).\n",
    "    - These changes ensure that the function behaves robustly across various input cases, providing users\n",
    "      with informative feedback and corrections as necessary.\n",
    "    \"\"\"\n",
    "\n",
    "    # Check if the dimension is a power of 2\n",
    "    if not np.log2(dim).is_integer():\n",
    "        raise ValueError(\"Dimension must be a power of 2\")\n",
    "\n",
    "    # Check if level is provided and valid\n",
    "    if level is None:\n",
    "        print(\"Level is not provided, setting level to log2(dim)\")\n",
    "        level = int(np.log2(dim))\n",
    "        print()\n",
    "    elif level < 1:\n",
    "        raise ValueError(\"Level should be an integer greater than 0, level=None => level=log2(dim)\")\n",
    "    elif level > int(np.log2(dim)):\n",
    "        print(\"Level provided is greater than max_level=log2(dim) => setting level to log2(dim)\")\n",
    "        level = int(np.log2(dim))\n",
    "\n",
    "    # Initialize the basis matrix\n",
    "    basis_matrix = np.zeros((dim, dim))\n",
    "    \n",
    "    for i in range(dim):\n",
    "        # Apply wavelet transform to each basis vector\n",
    "        coeffs = pywt.wavedec(data=np.eye(dim)[:, i], wavelet=wavelet, mode=mode, level=level, axis=0)\n",
    "        \n",
    "        # Flatten the coefficients and assign to the corresponding column in the basis matrix\n",
    "        basis_matrix[:, i] = np.hstack(coeffs)\n",
    "    \n",
    "    return basis_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test above code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute reduction of matrix to independent columns\n",
    "def compute_independent_columns(A, tol=1e-10):\n",
    "    \"\"\"\n",
    "    Computes the independent columns of a matrix using the QR decomposition.\n",
    "\n",
    "    Requires:\n",
    "    ----------\n",
    "    numpy as np\n",
    "\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    A : numpy array\n",
    "        The matrix for which to compute the independent columns.\n",
    "    tol : float, optional (default=1e-10)\n",
    "        The tolerance value for considering singular values as zero.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    ind_cols : numpy array\n",
    "        The matrix containing the independent columns of A.\n",
    "\n",
    "    Notes:\n",
    "    -----\n",
    "    - The function uses the QR decomposition to identify and select the independent columns of the matrix `A`.\n",
    "    - The rank of the matrix is determined by checking the diagonal elements of the `R` matrix from the QR decomposition.\n",
    "    - Columns corresponding to non-zero diagonal elements of `R` are considered independent.\n",
    "    \"\"\"\n",
    "    # Perform the QR decomposition\n",
    "    Q, R = np.linalg.qr(A)\n",
    "\n",
    "    # Find the independent columns based on the rank of R\n",
    "    rank = np.sum(np.abs(np.diagonal(R)) > tol)\n",
    "    ind_cols = A[:, :rank]\n",
    "\n",
    "    return ind_cols\n",
    "\n",
    "\n",
    "def check_normalization(A):\n",
    "    \"\"\"\n",
    "    Checks if the columns of a matrix are normalized (i.e., each column has a unit norm).\n",
    "\n",
    "    Requires:\n",
    "    ----------\n",
    "    numpy as np\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    A : numpy array\n",
    "        The matrix to check for normalization.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    is_normalized : bool\n",
    "        True if the columns of A are normalized, False otherwise.\n",
    "\n",
    "    Notes:\n",
    "    -----\n",
    "    - The function calculates the norm of each column in the matrix `A`.\n",
    "    - It then checks if all column norms are close to 1.0, which indicates normalization.\n",
    "    \"\"\"\n",
    "    column_norms = np.linalg.norm(A, axis=0)\n",
    "    is_normalized = np.allclose(column_norms, 1.0)\n",
    "    return is_normalized\n",
    "\n",
    "\n",
    "def compute_coherence(matrix):\n",
    "    \"\"\"\n",
    "    Computes the coherence of the given matrix.\n",
    "    \n",
    "    Parameters:\n",
    "    matrix (numpy.ndarray): An N x M matrix where coherence is to be calculated.\n",
    "    \n",
    "    Returns:\n",
    "    float: The coherence of the matrix.\n",
    "    \"\"\"\n",
    "    # Normalize the columns of the matrix\n",
    "    normalized_matrix = matrix / np.linalg.norm(matrix, axis=0, keepdims=True)\n",
    "    \n",
    "    # Compute the Gram matrix (inner products between all pairs of columns)\n",
    "    gram_matrix = np.dot(normalized_matrix.T, normalized_matrix)\n",
    "    \n",
    "    # Remove the diagonal elements (which are all 1's) to only consider distinct columns\n",
    "    np.fill_diagonal(gram_matrix, 0)\n",
    "    \n",
    "    # Compute the coherence as the maximum absolute value of the off-diagonal elements\n",
    "    coherence = np.max(np.abs(gram_matrix))\n",
    "    \n",
    "    return coherence\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIM = 16\n",
    "\n",
    "# Generate dictionary\n",
    "A = generate_dct_dictionary(DIM)\n",
    "print(f'Dictionary shape: {A.shape}')\n",
    "#printFormatted(Dict, decimals=4)\n",
    "print()\n",
    "\n",
    "# Orthonormality test\n",
    "print(\"Orthonormality test\")\n",
    "\n",
    "# Check orthogonality\n",
    "rankA = np.linalg.matrix_rank(A)\n",
    "#print(f'Rank of A: {rankA}')\n",
    "if A.shape[0] == rankA or A.shape[1] == rankA:\n",
    "    print(\"Matrix A is full rank.\")\n",
    "#print(f'Original shape: {A.shape}')\n",
    "#A = compute_independent_columns(A)\n",
    "#A = compute_independent_columns(A.T)\n",
    "#print(f'Full rank shape: {A.shape}')\n",
    "#printFormatted(A, decimals=4)\n",
    "\n",
    "# Check normalization\n",
    "#print()\n",
    "is_normalized = check_normalization(A)\n",
    "print(f\"Matrix A is normalized: {is_normalized}\")\n",
    "is_normalized = check_normalization(A.T)\n",
    "print(f\"Matrix A^T is normalized: {is_normalized}\")\n",
    "print()\n",
    "\n",
    "\n",
    "# Coherence test\n",
    "print(\"Coherence test\")\n",
    "coherence = compute_coherence(A)\n",
    "print(f'Coherence: {coherence}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sampling Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compressSignal(signal,Phi):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    \n",
    "    # length of signal block\n",
    "    N = Phi.shape[1]\n",
    "\n",
    "    # length of compressed block\n",
    "    M = Phi.shape[0]\n",
    "\n",
    "    # number of blocks\n",
    "    BLOCK_NUM = len(signal) // N\n",
    "\n",
    "    # each column of Y is the compressed version of a block of signal\n",
    "    Y = np.zeros((M, BLOCK_NUM))    \n",
    "    for i in range(BLOCK_NUM):\n",
    "        Y[:,i] = Phi @ signal[i*N:(i+1)*N]\n",
    "    \n",
    "    return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reconstruction method: Smooth-L0 (SL0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def SL0(y, A, sigma_min, sigma_decrease_factor=0.5, mu_0=2, L=3, A_pinv=None, showProgress=False):\n",
    "    \"\"\"\n",
    "    Returns the sparsest vector `s` that satisfies the underdetermined system of \n",
    "    linear equations `A @ s = y`, using the Smoothed L0 (SL0) algorithm.\n",
    "\n",
    "    Requires:\n",
    "    --------\n",
    "    - numpy as np\n",
    "    \n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    y : numpy array\n",
    "        The observed vector (Mx1), where M is the number of rows in `A`.\n",
    "    \n",
    "    A : numpy array\n",
    "        The measurement matrix (MxN), which should be 'wide', meaning it has more \n",
    "        columns than rows (N > M). The number of rows in `A` must match the length \n",
    "        of `y`.\n",
    "    \n",
    "    sigma_min : float\n",
    "        The minimum value of `sigma`, which determines the stopping criterion for \n",
    "        the algorithm. It should be chosen based on the noise level or desired \n",
    "        accuracy.\n",
    "    \n",
    "    sigma_decrease_factor : float, optional (default=0.5)\n",
    "        The factor by which `sigma` is decreased in each iteration. This should be \n",
    "        a positive value less than 1. Smaller values lead to quicker reduction of \n",
    "        `sigma`, possibly at the cost of accuracy for less sparse signals.\n",
    "    \n",
    "    mu_0 : float, optional (default=2)\n",
    "        The scaling factor for `mu`, where `mu = mu_0 * sigma^2`. This parameter \n",
    "        influences the convergence rate of the algorithm.\n",
    "    \n",
    "    L : int, optional (default=3)\n",
    "        The number of iterations for the inner loop (steepest descent). Increasing \n",
    "        `L` can improve the precision of the result but also increases computational \n",
    "        cost.\n",
    "    \n",
    "    A_pinv : numpy array, optional\n",
    "        The precomputed pseudoinverse of the matrix `A`. If not provided, it will be \n",
    "        calculated within the function as `np.linalg.pinv(A)`. Providing this value \n",
    "        is beneficial if the function is called repeatedly with the same `A`.\n",
    "    \n",
    "    showProgress : bool, optional (default=False)\n",
    "        If `True`, the function prints the current value of `sigma` during each \n",
    "        iteration, which helps monitor the convergence process.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    s : numpy array\n",
    "        The estimated sparse signal (Nx1) that best satisfies the equation `A @ s = y`.\n",
    "\n",
    "    Notes:\n",
    "    -----\n",
    "    - The algorithm works by iteratively reducing `sigma` in a geometric sequence, \n",
    "      starting with `sigma = 2 * max(abs(s))` and ending with `sigma_min`. At each \n",
    "      step, the function adjusts `s` to minimize the L0-norm by smoothing it using \n",
    "      a Gaussian kernel.\n",
    "    \n",
    "    - The choice of `sigma_min` is crucial: for noiseless cases, a smaller `sigma_min` \n",
    "      yields a sparser solution; for noisy cases, `sigma_min` should be a few times \n",
    "      the standard deviation of the noise in `s`.\n",
    "\n",
    "    - If `A_pinv` is precomputed and passed as an argument, the function becomes \n",
    "      more efficient, especially in scenarios where it is called repeatedly with the \n",
    "      same `A`.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if A_pinv is None:\n",
    "        A_pinv = np.linalg.pinv(A)\n",
    "\n",
    "    # Initialize the variables\n",
    "    s = A_pinv @ y\n",
    "    sigma = 2 * max(np.abs(s))\n",
    "\n",
    "    # Define lambda function for delta\n",
    "    OurDelta = lambda s, sigma: s * np.exp(-s**2 / sigma**2)\n",
    " \n",
    "    # Main loop\n",
    "    while sigma > sigma_min:\n",
    "        for i in range(L):\n",
    "            delta = OurDelta(s, sigma)\n",
    "            s = s - mu_0 * delta\n",
    "            s = s - A_pinv @ (A @ s - y)\n",
    "        \n",
    "        if showProgress:\n",
    "            print(f'sigma: {sigma}')\n",
    "\n",
    "        sigma = sigma * sigma_decrease_factor\n",
    "\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recovery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Non-kronecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_kron_recovery(Y, sigma_min, Phi, sigma_decrease_factor=0.5, mu_0=2, L=3, showProgress=False):\n",
    "\n",
    "\n",
    "    \n",
    "    n_block = Phi.shape[1]  # length of ORIGINAL signal block\n",
    "    Dict = generate_dct_dictionary(n_block)  # Dictionary\n",
    "    Theta = Phi @ Dict  # Theta\n",
    "    Theta_pinv = np.linalg.pinv(Theta)  # More-Penrose pseudoinverse of Theta\n",
    "    BLOCK_NUM = Y.shape[1]  # number of blocks\n",
    "\n",
    "    n_block = Theta.shape[1]  # length of ORIGINAL signal block\n",
    "\n",
    "    # Prepare output files\n",
    "    output_dir = 'debugCsvPy'  # Directory where CSV files will be stored\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    # Define file paths\n",
    "    y_block_path = os.path.join(output_dir, 'y_block.csv')\n",
    "    s_block_path = os.path.join(output_dir, 's_block.csv')\n",
    "\n",
    "    # Delete files if they exist\n",
    "    if os.path.exists(y_block_path):\n",
    "        os.remove(y_block_path)\n",
    "    if os.path.exists(s_block_path):\n",
    "        os.remove(s_block_path)\n",
    "\n",
    "    # Initialize the recovered signal\n",
    "    x_hat = np.zeros(BLOCK_NUM * n_block)  # Recovered signal will be as long as the original signal\n",
    "\n",
    "    # Process each block\n",
    "    for i in range(BLOCK_NUM):\n",
    "        y = Y[:, i]\n",
    "        s_block = SL0(y, Theta, sigma_min, sigma_decrease_factor, mu_0, L, Theta_pinv, showProgress)\n",
    "        x_hat[i*n_block:(i+1)*n_block] = Dict @ s_block\n",
    "\n",
    "        # Append y to y_block.csv\n",
    "        with open(y_block_path, 'a') as f_y:\n",
    "            np.savetxt(f_y, y.reshape(1, -1), delimiter=',', fmt='%.6f')\n",
    "\n",
    "        # Append s_block to s_block.csv\n",
    "        with open(s_block_path, 'a') as f_s:\n",
    "            np.savetxt(f_s, s_block.reshape(1, -1), delimiter=',', fmt='%.6f')\n",
    "        \n",
    "\n",
    "    # DEBUGGING\n",
    "    np.savetxt(f'{output_dir}/A1_Theta.csv', Theta, delimiter=',', fmt='%.6f')\n",
    "    np.savetxt(f'{output_dir}/dict1_Dict.csv', Dict, delimiter=',', fmt='%.6f')\n",
    "    np.savetxt(f'{output_dir}/A_pinv1.csv', Theta_pinv, delimiter=',', fmt='%.6f')\n",
    "\n",
    "\n",
    "    return x_hat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kronecker-based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def kron_recovery(Y, sigma_min, Phi, kron_factor, sigma_decrease_factor=0.5, mu_0=2, L=3, showProgress=False):\n",
    "\n",
    "\n",
    "\n",
    "    Phi_kron = np.kron(np.eye(kron_factor), Phi)  # KRONECKER product of Phi (kroncker measurement matrix)\n",
    "\n",
    "    n_block = Phi.shape[1]  # length of ORIGINAL signal block\n",
    "    #m_block = Phi.shape[0]  # length of compressed signal block\n",
    "    n_block_kron = Phi_kron.shape[1]  # length of ORIGINAL signal KRONECKER-block\n",
    "    #m_block_kron = Phi_kron.shape[0]  # length of compressed signal KRONECKER-block\n",
    "\n",
    "    BLOCK_NUM = Y.shape[1]  # number of blocks\n",
    "    N = n_block * BLOCK_NUM  # length of ORIGINAL signal\n",
    "    KRON_BLOCK_NUM = N // n_block_kron  # number of KRONECKER blocks\n",
    "\n",
    "    Dict_kron = generate_dct_dictionary(n_block_kron)  # Dictionary (kron)\n",
    "    Theta_kron = Phi_kron @ Dict_kron  # Theta (kron)\n",
    "    Theta_kron_pinv = np.linalg.pinv(Theta_kron)  # More-Penrose pseudoinverse of Theta (kron)\n",
    "\n",
    "\n",
    "    \n",
    "    # Prepare output files\n",
    "    output_dir = 'debugCsvPy'  # Directory where CSV files will be stored\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    # Define file paths\n",
    "    y_concatenated_path = os.path.join(output_dir, 'y_concatenated.csv')\n",
    "    s_block_kron_path = os.path.join(output_dir, 's_block_kron.csv')\n",
    "\n",
    "    # Delete files if they exist\n",
    "    if os.path.exists(y_concatenated_path):\n",
    "        os.remove(y_concatenated_path)\n",
    "    if os.path.exists(s_block_kron_path):\n",
    "        os.remove(s_block_kron_path)\n",
    "\n",
    "    # Initialize the recovered signal\n",
    "    x_hat_kron = np.zeros(N)\n",
    "\n",
    "    # Process each block\n",
    "    # DO NOT FORGET MATLAB IS COL-MAJOR, PY IS ROW-MAJOR, that darn 'F'... took my a day to figure out\n",
    "    for i in range(KRON_BLOCK_NUM):\n",
    "        y_kron = Y[:, i*kron_factor:(i+1)*kron_factor].reshape(-1, order='F')\n",
    "\n",
    "        # Print y_kron shape (for debugging)\n",
    "        #print(f'y_kron shape: {y_kron.shape}')\n",
    "        #print(y_kron)\n",
    "        #print('---')\n",
    "\n",
    "        s_kron_block = SL0(y_kron, Theta_kron, sigma_min, sigma_decrease_factor, mu_0, L, Theta_kron_pinv, showProgress)\n",
    "        x_hat_kron[n_block_kron*i:(i+1)*n_block_kron] = Dict_kron @ s_kron_block\n",
    "\n",
    "        # Append y_kron to y_concatenated.csv\n",
    "        with open(y_concatenated_path, 'a') as f_y_kron:\n",
    "            np.savetxt(f_y_kron, y_kron.reshape(1, -1), delimiter=',', fmt='%.6f')\n",
    "            # Write a line of zeros to separate blocks\n",
    "            np.savetxt(f_y_kron, np.zeros_like(y_kron.reshape(1, -1)), delimiter=',', fmt='%.6f')\n",
    "\n",
    "        # Append s_kron_block to s_block_kron.csv\n",
    "        with open(s_block_kron_path, 'a') as f_s_kron:\n",
    "            np.savetxt(f_s_kron, s_kron_block.reshape(1, -1), delimiter=',', fmt='%.6f')\n",
    "            # Write a line of zeros to separate blocks\n",
    "            np.savetxt(f_s_kron, np.zeros_like(s_kron_block.reshape(1, -1)), delimiter=',', fmt='%.6f')\n",
    "\n",
    "    # DEBUGGING\n",
    "    np.savetxt(f'{output_dir}/AA_Phi_kron.csv', Phi_kron, delimiter=',', fmt='%.6f')\n",
    "    np.savetxt(f'{output_dir}/A2_Theta_kron.csv', Theta_kron, delimiter=',', fmt='%.6f')\n",
    "    np.savetxt(f'{output_dir}/dict2_Dict_kron.csv', Dict_kron, delimiter=',', fmt='%.6f')\n",
    "    np.savetxt(f'{output_dir}/A_pinv2.csv', Theta_kron_pinv, delimiter=',', fmt='%.6f')\n",
    "    \n",
    "\n",
    "\n",
    "    return x_hat_kron\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation of results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_snr(signal, recovered_signal):\n",
    "    \"\"\"\n",
    "    Calculates the Signal-to-Noise Ratio (SNR) between the original signal and the recovered signal.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    signal : numpy array\n",
    "        The original signal.\n",
    "    recovered_signal : numpy array\n",
    "        The recovered signal after some processing or recovery algorithm.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    snr : float\n",
    "        The Signal-to-Noise Ratio (SNR) in decibels (dB).\n",
    "\n",
    "    Notes:\n",
    "    -----\n",
    "    - The SNR is calculated as 20 * log10(norm(original_signal) / norm(original_signal - recovered_signal)).\n",
    "    - A higher SNR value indicates a better recovery, with less error relative to the original signal.\n",
    "    \"\"\"\n",
    "    error = recovered_signal - signal\n",
    "    snr = 20 * np.log10(np.linalg.norm(signal) / np.linalg.norm(error))\n",
    "    \n",
    "    return snr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot recovered over original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_signals(original_signal, reconstructed_signal, original_name=\"Original Signal\", reconstructed_name=\"Reconstructed Signal\"):\n",
    "    \"\"\"\n",
    "    Plots the original signal and the reconstructed signal on the same plot with the given names.\n",
    "\n",
    "    Requires:\n",
    "    ----------\n",
    "    - matplotlib.pyplot as plt\n",
    "    - numpy as np\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    original_signal : numpy array\n",
    "        The original signal to be plotted.\n",
    "    \n",
    "    reconstructed_signal : numpy array\n",
    "        The reconstructed signal to be plotted.\n",
    "    \n",
    "    original_name : str, optional (default=\"Original Signal\")\n",
    "        The name to display for the original signal in the plot.\n",
    "    \n",
    "    reconstructed_name : str, optional (default=\"Reconstructed Signal\")\n",
    "        The name to display for the reconstructed signal in the plot.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ensure the signals have the same length\n",
    "    if len(original_signal) != len(reconstructed_signal):\n",
    "        raise ValueError(\"The original signal and the reconstructed signal must have the same length.\")\n",
    "    \n",
    "    # Create the plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(original_signal, label=original_name, color='blue', linewidth=1.5)\n",
    "    plt.plot(reconstructed_signal, label=reconstructed_name, color='red', linestyle='--', linewidth=1.5)\n",
    "    plt.title(f\"{original_name} vs {reconstructed_name}\")\n",
    "    plt.xlabel('Sample Index')\n",
    "    plt.ylabel('Amplitude')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # DEBUGGING PURPUSES\n",
    "    import os\n",
    "    # Directory where you want to save the CSV file\n",
    "    output_dir = 'debugCsvPy'  # global var that refers to the directory where the CSV file will be saved\n",
    "    # Create the directory if it doesn't exist\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "\n",
    "    # I put this here because you have to decide kron fact based on\n",
    "    # the signal length, but for testing reasons I prefer to do the \n",
    "    # opposite: choose signal length based on kron factor\n",
    "    n_block = 16 # block size, deafult is 16\n",
    "    kron_factor = n_block * 2  # kron factor default is 32\n",
    "\n",
    "\n",
    "    ### PREPARE\n",
    "    # ----------------------------------------------------------------\n",
    "\n",
    "    ## DATA\n",
    "    #  upload data\n",
    "    data = scipy.io.loadmat('100m.mat')\n",
    "    # retrieve the key to a string\n",
    "    key = list(data.keys())[0]\n",
    "    # retrieve the values\n",
    "    signal = data[key][0,:]  # [0 or 1, 0:650000] s.t. first dim: (0 is MLII, 1 is V5)\n",
    "    num = 10  # how many kronecker blocks will be long the original signals\n",
    "    temp = n_block * kron_factor  # this is equal to the length of a kronecker block\n",
    "    start = int(temp * 0)  # choose where to start our signal in the record (signal is a piece of the record)\n",
    "    end = int(start + temp * num)\n",
    "    signal = signal[start:end]  # comment out to use the whole signal\n",
    "    # print the shape of the signal\n",
    "    print(f'Signal shape: {signal.shape}') \n",
    "    # plot the signal\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(signal)\n",
    "    plt.title('ECG Signal')\n",
    "    plt.xlabel('Index')\n",
    "    plt.ylabel('Amplitude')\n",
    "    plt.grid(True)\n",
    "    plt.show()    \n",
    "\n",
    "    # DEBUGGING\n",
    "    # save the matrix to a CSV file\n",
    "    np.savetxt(os.path.join(output_dir, 'signal.csv'), signal, delimiter=',', fmt='%.6f')\n",
    "\n",
    "\n",
    "    ## PARAMETERS\n",
    "    # compression ratio\n",
    "    CR = 1/4 \n",
    "    # non-kronecker\n",
    "    #n_block = 16 # block size`\n",
    "    m_block = int(n_block * CR) # compressed block size\n",
    "    # kronecker\n",
    "    #kron_factor = 32  # kron factor\n",
    "    n_block_kron = n_block * kron_factor  # kron block size\n",
    "    m_block_kron = int(n_block_kron * CR) # compressed kron block size\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    ### SAMPLING PHASE \n",
    "    # ----------------------------------------------------------------\n",
    "\n",
    "\n",
    "    ## MEASUREMENT MATRIX\n",
    "    # generate the measurement matrix\n",
    "    Phi = generate_DBDD_matrix(m_block, n_block)\n",
    "    #Phi = generate_random_matrix(m_block, n_block, matrix_type='gaussian')\n",
    "    #Phi = generate_random_matrix(m_block, n_block, matrix_type='scaled_binary')\n",
    "    #Phi = generate_random_matrix(m_block, n_block, matrix_type='unscaled_binary')\n",
    "\n",
    "\n",
    "    # DEBUGGING\n",
    "    # save the matrix to a CSV file\n",
    "    np.savetxt(os.path.join(output_dir, 'A_Phi.csv'), Phi, delimiter=',', fmt='%.6f')\n",
    "\n",
    "\n",
    "    ## COMPRESS THE SIGNAL\n",
    "    # compress the signal\n",
    "    Y = compressSignal(signal, Phi)\n",
    "\n",
    "    # DEBUGGING\n",
    "    # save the matrix to a CSV file\n",
    "    np.savetxt(os.path.join(output_dir, 'Y.csv'), Y, delimiter=',', fmt='%.6f')\n",
    "\n",
    "\n",
    "\n",
    "    ### RECOVERY PHASE\n",
    "    # ----------------------------------------------------------------\n",
    "\n",
    "\n",
    "    ## SL0 PARAMETERS\n",
    "    sigma_off = 0.001  # offset for sigma\n",
    "    mu_0 = 2  # scaling factor for mu\n",
    "    sigma_decrease_factor = 0.5  # factor for decreasing sigma\n",
    "    L = 3  # number of iterations for the inner loop\n",
    "    if sigma_off > 0:\n",
    "        sigma_min = sigma_off * 4  # minimum value of sigma\n",
    "    else:\n",
    "        sigma_min = 0.00001  # minimum value of sigma\n",
    "\n",
    "\n",
    "\n",
    "    ## RECOVERY non-KRONECKER\n",
    "    # recover the signal\n",
    "    recovered_signal = non_kron_recovery(Y=Y, sigma_min=sigma_min, Phi=Phi, sigma_decrease_factor=sigma_decrease_factor,\n",
    "                                            mu_0=mu_0, L=L, showProgress=False)\n",
    "    \n",
    "\n",
    "    ## RECOVERY KRONECKER\n",
    "    # recover the signal\n",
    "\n",
    "    recovered_signal_kron = kron_recovery(Y=Y, sigma_min=sigma_min, Phi=Phi, sigma_decrease_factor=sigma_decrease_factor,\n",
    "                                            mu_0=mu_0, L=L, showProgress=False, kron_factor=kron_factor)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ### EVALUATION\n",
    "    # ----------------------------------------------------------------\n",
    "\n",
    "\n",
    "    ## PLOT\n",
    "    # plot the signals\n",
    "    plot_signals(signal, recovered_signal, original_name=\"Original Signal\", reconstructed_name=\"Recovered Signal\")\n",
    "\n",
    "    ## SNR\n",
    "    # calculate the SNR\n",
    "    snr = calculate_snr(signal, recovered_signal)\n",
    "    print(f'SNR: {snr} dB')\n",
    "\n",
    "    ## PLOT KRON\n",
    "    # plot the signals\n",
    "    plot_signals(signal, recovered_signal_kron, original_name=\"Original Signal\", reconstructed_name=\"Recovered Signal (Kronecker)\")\n",
    "    \n",
    "\n",
    "    ## SNR KRON\n",
    "    # calculate the SNR\n",
    "    snr_kron = calculate_snr(signal, recovered_signal_kron)\n",
    "    print(f'SNR (Kronecker): {snr_kron} dB')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check if variables are the same as the ones in MATLAB\n",
    "During the run all major variables and vectors were saved as csv, the same is done in the MATLAB code, so that we can compare them. This was especially helpful to find bugs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def compare_csv_matrices(dir1, dir2):\n",
    "    # List CSV files in both directories\n",
    "    files_dir1 = set(f for f in os.listdir(dir1) if f.endswith('.csv'))\n",
    "    files_dir2 = set(f for f in os.listdir(dir2) if f.endswith('.csv'))\n",
    "    \n",
    "    # Find common files\n",
    "    common_files = files_dir1.intersection(files_dir2)\n",
    "    \n",
    "    # Report if any files are missing in either directory\n",
    "    missing_in_dir1 = files_dir2 - files_dir1\n",
    "    missing_in_dir2 = files_dir1 - files_dir2\n",
    "    \n",
    "    if missing_in_dir1:\n",
    "        print(f\"Files missing in {dir1}: {', '.join(missing_in_dir1)}\")\n",
    "    if missing_in_dir2:\n",
    "        print(f\"Files missing in {dir2}: {', '.join(missing_in_dir2)}\")\n",
    "    \n",
    "    # Compare contents of common files\n",
    "    mismatches = []\n",
    "    for filename in common_files:\n",
    "        path1 = os.path.join(dir1, filename)\n",
    "        path2 = os.path.join(dir2, filename)\n",
    "        \n",
    "        # Read CSV files\n",
    "        df1 = pd.read_csv(path1, header=None)\n",
    "        df2 = pd.read_csv(path2, header=None)\n",
    "        \n",
    "        # Compare as numpy arrays\n",
    "        if not np.array_equal(df1.values, df2.values):\n",
    "            mismatches.append(filename)\n",
    "    \n",
    "    # Report results\n",
    "    if mismatches:\n",
    "        print(\"Files with mismatched content:\")\n",
    "        for filename in mismatches:\n",
    "            print(f\"- {filename}\")\n",
    "    else:\n",
    "        print(\"All files match!\")\n",
    "\n",
    "# Example usage\n",
    "compare_csv_matrices('../debugMatlab/debugCsvMAT', './debugCsvPy')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".namlVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
