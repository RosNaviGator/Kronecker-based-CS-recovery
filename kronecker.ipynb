{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kronecker-based Compressed Sensing signal recovery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we explore the _compression_ and _recovery_ of aCompressive Sensing _compressible_ signal through __Compressive Sensing__ with __Kronecker Technique__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printFormatted(matrix, decimals=4):\n",
    "    \"\"\"\n",
    "    Prints the matrix with formatted elements aligned in columns for improved readability.\n",
    "\n",
    "    Requires:\n",
    "    ----------\n",
    "    numpy as np\n",
    "    \n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    matrix : numpy array\n",
    "        The matrix to be printed.\n",
    "    decimals : int, optional (default=4)\n",
    "        The number of decimal places for formatting the elements.\n",
    "\n",
    "    Notes:\n",
    "    -----\n",
    "    - The function aligns columns based on the maximum width needed for the formatted elements, ensuring the matrix is displayed neatly.\n",
    "    - This function is useful for visual inspection of numerical matrices, especially those with varying magnitudes.\n",
    "    \"\"\"\n",
    "    # Determine the maximum width needed to keep alignment\n",
    "    max_width = max(len(f'{value:.{decimals}f}') for row in matrix for value in row)\n",
    "\n",
    "    # Create a formatted string for each element in the matrix, ensuring alignment\n",
    "    formatted_matrix = '\\n'.join([' '.join([f'{value:>{max_width}.{decimals}f}' for value in row]) for row in matrix])\n",
    "\n",
    "    # Print the formatted matrix\n",
    "    print(formatted_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a random sparse signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sparseSignal(N, K=None, sigma_inactive=0.01, sigma_active=0.5, fixedActiveValue=None):\n",
    "    \"\"\"\n",
    "    Generates a K-sparse signal with N-K inactive components, chosen randomly. The inactive components \n",
    "    are nearly zero, while the active components are either a fixed value or generated from a Gaussian\n",
    "    distribution.\n",
    "\n",
    "    Requires:\n",
    "    ----------\n",
    "    numpy as np\n",
    "    \n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    N : int\n",
    "        The total number of components in the signal.\n",
    "    \n",
    "    K : int, optional\n",
    "        The number of active (non-zero) components in the signal. If not provided, defaults to 10% of `N`.\n",
    "    \n",
    "    sigma_inactive : float, optional (default=0.01)\n",
    "        The standard deviation of the Gaussian noise added to the inactive components.\n",
    "    \n",
    "    sigma_active : float, optional (default=0.5)\n",
    "        The standard deviation of the Gaussian noise for generating the active components, if `fixedActiveValue` \n",
    "        is not provided.\n",
    "    \n",
    "    fixedActiveValue : float, optional\n",
    "        If provided, this fixed value is assigned to all active components instead of generating them randomly.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    signal : numpy array\n",
    "        The generated signal of length `N` with `K` active components.\n",
    "    \n",
    "    active_indices : numpy array\n",
    "        The indices of the active components in the signal.\n",
    "\n",
    "    Notes:\n",
    "    -----\n",
    "    - The signal is constructed by first randomly selecting `K` indices as active components.\n",
    "    - If `fixedActiveValue` is `None`, the active components are drawn from a Gaussian distribution \n",
    "      with standard deviation `sigma_active`.\n",
    "    - Gaussian noise with standard deviation `sigma_inactive` is then added to the inactive components, \n",
    "      ensuring they have small random values near zero.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if K is None:\n",
    "        N = int(N)\n",
    "        K = int(0.1 * N)\n",
    "    else:\n",
    "        N = int(N)\n",
    "        K = int(K)\n",
    "\n",
    "    active_indexes = np.zeros(N)\n",
    "    active_indexes[:K] = 1\n",
    "    np.random.shuffle(active_indexes)\n",
    "\n",
    "    signal = np.zeros(N)\n",
    "    \n",
    "    if fixedActiveValue is None:\n",
    "        # Generate active components with Gaussian noise\n",
    "        signal[active_indexes == 1] = np.random.randn(K) * sigma_active\n",
    "    else:\n",
    "        # Use fixed value for active components\n",
    "        signal[active_indexes == 1] = fixedActiveValue\n",
    "    \n",
    "    # Add Gaussian noise only to inactive components\n",
    "    signal[active_indexes == 0] += np.random.randn(N - K) * sigma_inactive\n",
    "\n",
    "    return signal, np.where(active_indexes == 1)[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the above function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Parameters for the sparse signal\n",
    "N = 2 ** 10\n",
    "K = int(0.1 * N)\n",
    "sigma_noise = 0.01\n",
    "sigma_active = 0.2\n",
    "fixedActiveValue = None\n",
    "\n",
    "# Generate the sparse signal\n",
    "x, active_indices = sparseSignal(N, K, sigma_noise, sigma_active, fixedActiveValue)\n",
    "\n",
    "# Display the full signal array (optional, since it's very large)\n",
    "#np.set_printoptions(threshold=np.inf)  # Setting the threshold to infinity to display the full array\n",
    "#print(\"Signal: \", x)\n",
    "#print(\"Active indices: \", active_indices)\n",
    "\n",
    "# Print the maximum value in the signal\n",
    "print(\"Maximum value in the signal:\", np.max(x))\n",
    "\n",
    "# Plot the sparse signal\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x)\n",
    "plt.title('Sparse Signal with Added Noise')\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Measurement matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def generate_DBDD_matrix(M, N):\n",
    "    \"\"\"\n",
    "    Generates a deterministic Diagonally Blocked Block Diagonal (DBBD) matrix.\n",
    "\n",
    "    Requires:\n",
    "    ----------\n",
    "    numpy as np\n",
    "\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    M : int\n",
    "        Number of rows in the matrix.\n",
    "    N: int\n",
    "        Number of columns in the matrix. Should be a multiple of M.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    A : numpy array\n",
    "        The generated DBBD matrix of shape (M, N).\n",
    "    \"\"\"\n",
    "    \n",
    "    if N % M != 0:\n",
    "        raise ValueError(\"N should be a multiple of M.\")\n",
    "    \n",
    "    Phi = np.zeros((M, N))\n",
    "    m = N // M\n",
    "    \n",
    "    for i in range(M):\n",
    "        Phi[i, i*m:(i+1)*m] = 1\n",
    "\n",
    "    return Phi\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def generate_random_matrix(M, N, matrix_type='gaussian'):\n",
    "    \"\"\"\n",
    "    Generates a random matrix based on the specified type.\n",
    "\n",
    "    Requires:\n",
    "    ----------\n",
    "    numpy as np\n",
    "    \n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    M : int\n",
    "        Number of rows in the matrix.\n",
    "    N : int\n",
    "        Number of columns in the matrix.\n",
    "    matrix_type : str, optional (default='gaussian')\n",
    "        The type of random matrix to generate. Options are:\n",
    "        - 'gaussian': A matrix with entries drawn from a normal distribution.\n",
    "        - 'scaled_binary': A matrix with binary entries (±0.5), scaled by 1/sqrt(M).\n",
    "        - 'unscaled_binary': A matrix with binary entries (±1), with no scaling.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    A : numpy array\n",
    "        The generated random matrix of shape (M, N).\n",
    "    \"\"\"\n",
    "    \n",
    "    if matrix_type == 'gaussian':\n",
    "        A = ((1/M)**2) * np.random.randn(M, N)\n",
    "\n",
    "    elif matrix_type == 'scaled_binary':\n",
    "        A = np.random.binomial(1, 0.5, size=(M, N)) - 0.5\n",
    "        A = (1/np.sqrt(M)) * A\n",
    "\n",
    "    elif matrix_type == 'unscaled_binary':\n",
    "        A = np.random.binomial(1, 0.5, size=(M, N)) * 2 - 1\n",
    "        # This generates -1 or +1 directly without scaling\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported matrix type. Choose either 'gaussian', 'scaled_binary', or 'unscaled_binary'.\")\n",
    "\n",
    "    return A\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test above code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 2 ** 4\n",
    "M = N // 4\n",
    "\n",
    "# Generate a deterministic DBBD matrix\n",
    "A = generate_DBDD_matrix(M, N)\n",
    "print(f'DBBD:\\n{A}\\n\\n')\n",
    "\n",
    "# Generate a random Gaussian matrix\n",
    "B = generate_random_matrix(M, N, matrix_type='gaussian')\n",
    "print(f'gaussian:\\n{B}\\n\\n')\n",
    "\n",
    "# Generate a random scaled binary matrix\n",
    "C = generate_random_matrix(M, N, matrix_type='scaled_binary')\n",
    "print(f'scaled binary:\\n{C}\\n\\n')\n",
    "\n",
    "# Generate a random unscaled binary matrix\n",
    "D = generate_random_matrix(M, N, matrix_type='unscaled_binary')\n",
    "print(f'unscaled binary:\\n{D}\\n\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DCT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.fftpack as fftpack\n",
    "\n",
    "def generate_dct_dictionary(N):\n",
    "    \"\"\"\n",
    "    Generates a Discrete Cosine Transform (DCT) basis dictionary.\n",
    "\n",
    "    Requires:\n",
    "    ----------\n",
    "    numpy as np\n",
    "    scipy.fftpack as fftpack\n",
    "\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    N : int\n",
    "        The size of the dictionary (i.e., the length of the signal).\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    dict_matrix : numpy array\n",
    "        The generated DCT dictionary matrix of shape (N, N).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate a DCT basis dictionary\n",
    "    dict_matrix = fftpack.dct(np.eye(N), norm='ortho')\n",
    "    \n",
    "    return dict_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Haar wavelet dictionary, BEWARE: it's orthogonal but __not normalized__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def generate_haar_basis(dim):\n",
    "    \"\"\"\n",
    "    Generates a Haar orthogonal basis matrix of given dimension.\n",
    "\n",
    "    Requires:\n",
    "    ----------\n",
    "    numpy as np\n",
    "  \n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dim : int\n",
    "        The dimension of the basis (i.e., the length of the signal and the size of the matrix).\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    haar_basis : numpy array\n",
    "        The generated Haar orthogonal basis matrix of shape (dim, dim).\n",
    "\n",
    "    Raises:\n",
    "    -------\n",
    "    ValueError\n",
    "        If the dimension `dim` is not a power of 2.\n",
    "\n",
    "    Notes:\n",
    "    -----\n",
    "    - This implementation generates an orthogonal Haar basis, where the first row represents\n",
    "      the DC component (all ones, scaled by the square root of the dimension). \n",
    "    - The subsequent rows are Haar wavelet-like functions with alternating positive and \n",
    "      negative values.\n",
    "    - While the basis is orthogonal (i.e., the inner product of any two distinct rows is zero),\n",
    "      it is not orthonormal because the rows are not necessarily unit vectors.\n",
    "    - Additionally, this approach does not guarantee sparsity in the basis, which is often \n",
    "      desired in wavelet transforms for signal compression or denoising.\n",
    "\n",
    "    Warning:\n",
    "    -------\n",
    "    - Using this manually constructed Haar basis is generally not advisable. The traditional\n",
    "      Haar wavelet basis generated using wavelet libraries like PyWavelets (pywt) is not only\n",
    "      orthonormal but also designed to promote sparsity in signal representation.\n",
    "    - For a more efficient and normalized Haar basis, consider using the `generate_dwt_basis`\n",
    "      function with the option `wavelet='haar'`. This ensures a properly normalized and \n",
    "      sparse basis suitable for various signal processing applications.\n",
    "    \"\"\"\n",
    "\n",
    "    # Check if the dimension is a power of 2\n",
    "    if not np.log2(dim).is_integer():\n",
    "        raise ValueError(\"Dimension must be a power of 2\")\n",
    "\n",
    "    # Initialize the Haar basis matrix\n",
    "    haar_basis = np.zeros((dim, dim))\n",
    "\n",
    "    # Set the first row of the Haar basis matrix (DC component)\n",
    "    haar_basis[0, :] = 1 / np.sqrt(dim)\n",
    "\n",
    "    # Generate the remaining rows of the Haar basis matrix\n",
    "    for i in range(1, dim):\n",
    "        k = int(np.log2(i))\n",
    "        p = i - 2 ** k\n",
    "        q = 2 ** (k + 1)\n",
    "\n",
    "        for j in range(dim):\n",
    "            if p < q / 2:\n",
    "                haar_basis[i, j] = np.sqrt(2) / np.sqrt(q)\n",
    "            else:\n",
    "                haar_basis[i, j] = -np.sqrt(2) / np.sqrt(q)\n",
    "\n",
    "            p += 1\n",
    "            if p == q:\n",
    "                p = 0\n",
    "\n",
    "    return haar_basis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ANY wavelet__ dictionary, choose which __wavelet__ and __level__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pywt\n",
    "\n",
    "def generate_dwt_basis(dim, wavelet='db5', mode='per', level=None):\n",
    "    \"\"\"\n",
    "    Generates a wavelet orthonormal basis matrix using the Discrete Wavelet Transform (DWT).\n",
    "\n",
    "    Requires:\n",
    "    --------\n",
    "    - numpy (as np)\n",
    "    - pywt (PyWavelets)\n",
    "    \n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dim : int\n",
    "        The dimension of the basis (i.e., the length of the signal and the size of the matrix).\n",
    "    \n",
    "    wavelet : str, optional (default='db5')\n",
    "        The name of the wavelet to use. Options include Daubechies wavelets ('db1' to 'db20') and\n",
    "        other wavelet families available in PyWavelets.\n",
    "\n",
    "    mode : str, optional (default='per')\n",
    "        The signal extension mode to use when applying the wavelet transform. The default is\n",
    "        'per' for periodic extension. Other options include 'zero', 'symmetric', etc.\n",
    "\n",
    "    level : int, optional\n",
    "        The level of decomposition to perform. If not provided (level=None), the function sets\n",
    "        the level to log2(dim). If a value is provided, it should be an integer greater than 0.\n",
    "        If the provided level is greater than log2(dim), the function resets it to log2(dim).\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    basis_matrix : numpy array\n",
    "        The generated wavelet basis matrix of shape (dim, dim). Each column of this matrix \n",
    "        represents a wavelet basis vector for the given wavelet and decomposition level.\n",
    "\n",
    "    Raises:\n",
    "    -------\n",
    "    ValueError\n",
    "        If `dim` is not a power of 2, or if `level` is not an integer greater than 0.\n",
    "\n",
    "    Notes:\n",
    "    -----\n",
    "    - This function assumes that `dim` is a power of 2. If it is not, the function raises a ValueError.\n",
    "    - If the decomposition level is not provided, it defaults to log2(dim), ensuring a full decomposition.\n",
    "    - The function constructs the basis by applying the wavelet transform to each standard basis vector\n",
    "      (i.e., the columns of the identity matrix) and concatenating the resulting coefficients into the\n",
    "      final basis matrix.\n",
    "\n",
    "    Changes Made:\n",
    "    -------------\n",
    "    - Added validation for `dim` to ensure it is a power of 2.\n",
    "    - Added checks for the `level` parameter to ensure it is a positive integer and does not exceed log2(dim).\n",
    "    - The function now defaults to log2(dim) if the level is not provided or is greater than log2(dim).\n",
    "    - These changes ensure that the function behaves robustly across various input cases, providing users\n",
    "      with informative feedback and corrections as necessary.\n",
    "    \"\"\"\n",
    "\n",
    "    # Check if the dimension is a power of 2\n",
    "    if not np.log2(dim).is_integer():\n",
    "        raise ValueError(\"Dimension must be a power of 2\")\n",
    "\n",
    "    # Check if level is provided and valid\n",
    "    if level is None:\n",
    "        print(\"Level is not provided, setting level to log2(dim)\")\n",
    "        level = int(np.log2(dim))\n",
    "        print()\n",
    "    elif level < 1:\n",
    "        raise ValueError(\"Level should be an integer greater than 0, level=None => level=log2(dim)\")\n",
    "    elif level > int(np.log2(dim)):\n",
    "        print(\"Level provided is greater than max_level=log2(dim) => setting level to log2(dim)\")\n",
    "        level = int(np.log2(dim))\n",
    "\n",
    "    # Initialize the basis matrix\n",
    "    basis_matrix = np.zeros((dim, dim))\n",
    "    \n",
    "    for i in range(dim):\n",
    "        # Apply wavelet transform to each basis vector\n",
    "        coeffs = pywt.wavedec(data=np.eye(dim)[:, i], wavelet=wavelet, mode=mode, level=level, axis=0)\n",
    "        \n",
    "        # Flatten the coefficients and assign to the corresponding column in the basis matrix\n",
    "        basis_matrix[:, i] = np.hstack(coeffs)\n",
    "    \n",
    "    return basis_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function that calls the previous to generate the desired dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_basis(DIM, basis_type='dct', wavelet='db5', mode='per', level=None):\n",
    "    \"\"\"\n",
    "    Generates a basis matrix based on the specified type.\n",
    "\n",
    "    Requires:\n",
    "    ----------\n",
    "    generate_dct_dictionary\n",
    "    generate_haar_basis\n",
    "    generate_dwt_basis\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    DIM : int\n",
    "        The dimension of the basis (i.e., the length of the signal).\n",
    "    \n",
    "    basis_type : str, optional (default='dct')\n",
    "        The type of basis to generate. Options are:\n",
    "        - 'dct' for Discrete Cosine Transform basis\n",
    "        - 'haar_classic' for Haar wavelet basis\n",
    "        - 'dwt' for Daubechies wavelet basis\n",
    "\n",
    "    wavelet : str, optional (default='db5')\n",
    "        The name of the Daubechies wavelet to use if `basis_type='dwt'`.\n",
    "\n",
    "    mode : str, optional (default='per')\n",
    "        The extension mode to use if `basis_type='dwt'`.\n",
    "\n",
    "    level : int or None, optional (default=None)\n",
    "        The level of decomposition to use if `basis_type='dwt'`.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    basis_matrix : numpy array\n",
    "        The generated basis matrix of shape (DIM, DIM).\n",
    "    \"\"\"\n",
    "    \n",
    "    if basis_type == 'dct':\n",
    "        return generate_dct_dictionary(DIM)\n",
    "    \n",
    "    elif basis_type == 'haar_classic':\n",
    "        return generate_haar_basis(DIM)\n",
    "    \n",
    "    elif basis_type == 'dwt':\n",
    "        return generate_dwt_basis(DIM, wavelet=wavelet, mode=mode, level=level)\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown basis_type '{basis_type}'. Supported types are 'dct', 'haar_classic', and 'dwt'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test above code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute reduction of matrix to independent columns\n",
    "def compute_independent_columns(A, tol=1e-10):\n",
    "    \"\"\"\n",
    "    Computes the independent columns of a matrix using the QR decomposition.\n",
    "\n",
    "    Requires:\n",
    "    ----------\n",
    "    numpy as np\n",
    "\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    A : numpy array\n",
    "        The matrix for which to compute the independent columns.\n",
    "    tol : float, optional (default=1e-10)\n",
    "        The tolerance value for considering singular values as zero.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    ind_cols : numpy array\n",
    "        The matrix containing the independent columns of A.\n",
    "\n",
    "    Notes:\n",
    "    -----\n",
    "    - The function uses the QR decomposition to identify and select the independent columns of the matrix `A`.\n",
    "    - The rank of the matrix is determined by checking the diagonal elements of the `R` matrix from the QR decomposition.\n",
    "    - Columns corresponding to non-zero diagonal elements of `R` are considered independent.\n",
    "    \"\"\"\n",
    "    # Perform the QR decomposition\n",
    "    Q, R = np.linalg.qr(A)\n",
    "\n",
    "    # Find the independent columns based on the rank of R\n",
    "    rank = np.sum(np.abs(np.diagonal(R)) > tol)\n",
    "    ind_cols = A[:, :rank]\n",
    "\n",
    "    return ind_cols\n",
    "\n",
    "\n",
    "def check_normalization(A):\n",
    "    \"\"\"\n",
    "    Checks if the columns of a matrix are normalized (i.e., each column has a unit norm).\n",
    "\n",
    "    Requires:\n",
    "    ----------\n",
    "    numpy as np\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    A : numpy array\n",
    "        The matrix to check for normalization.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    is_normalized : bool\n",
    "        True if the columns of A are normalized, False otherwise.\n",
    "\n",
    "    Notes:\n",
    "    -----\n",
    "    - The function calculates the norm of each column in the matrix `A`.\n",
    "    - It then checks if all column norms are close to 1.0, which indicates normalization.\n",
    "    \"\"\"\n",
    "    column_norms = np.linalg.norm(A, axis=0)\n",
    "    is_normalized = np.allclose(column_norms, 1.0)\n",
    "    return is_normalized\n",
    "\n",
    "\n",
    "def compute_coherence(matrix):\n",
    "    \"\"\"\n",
    "    Computes the coherence of the given matrix.\n",
    "    \n",
    "    Parameters:\n",
    "    matrix (numpy.ndarray): An N x M matrix where coherence is to be calculated.\n",
    "    \n",
    "    Returns:\n",
    "    float: The coherence of the matrix.\n",
    "    \"\"\"\n",
    "    # Normalize the columns of the matrix\n",
    "    normalized_matrix = matrix / np.linalg.norm(matrix, axis=0, keepdims=True)\n",
    "    \n",
    "    # Compute the Gram matrix (inner products between all pairs of columns)\n",
    "    gram_matrix = np.dot(normalized_matrix.T, normalized_matrix)\n",
    "    \n",
    "    # Remove the diagonal elements (which are all 1's) to only consider distinct columns\n",
    "    np.fill_diagonal(gram_matrix, 0)\n",
    "    \n",
    "    # Compute the coherence as the maximum absolute value of the off-diagonal elements\n",
    "    coherence = np.max(np.abs(gram_matrix))\n",
    "    \n",
    "    return coherence\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for DIM in range(1):\n",
    "    \n",
    "    DIM = 16  # DIM must be a power of 2\n",
    "    \n",
    "    print(f'DIM: {DIM}')\n",
    "\n",
    "\n",
    "    for test in range(1):\n",
    "        # Generate dictionary\n",
    "        Dict = generate_basis(DIM, basis_type='dwt', wavelet='db10', mode='per', level=None)\n",
    "        print(f'Dictionary shape: {Dict.shape}, test: {test}')\n",
    "        #printFormatted(Dict, decimals=4)\n",
    "        print()\n",
    "\n",
    "\n",
    "        # Orthonormality test\n",
    "        print(\"Orthonormality test\")\n",
    "        A = Dict\n",
    "\n",
    "        # Check orthogonality\n",
    "        rankA = np.linalg.matrix_rank(A)\n",
    "        #print(f'Rank of A: {rankA}')\n",
    "        if A.shape[0] == rankA or A.shape[1] == rankA:\n",
    "            print(\"Matrix A is full rank.\")\n",
    "        #print(f'Original shape: {A.shape}')\n",
    "        #A = compute_independent_columns(A)\n",
    "        #A = compute_independent_columns(A.T)\n",
    "        #print(f'Full rank shape: {A.shape}')\n",
    "        #printFormatted(A, decimals=4)\n",
    "\n",
    "        # Check normalization\n",
    "        #print()\n",
    "        is_normalized = check_normalization(A)\n",
    "        print(f\"Matrix A is normalized: {is_normalized}\")\n",
    "        is_normalized = check_normalization(A.T)\n",
    "        print(f\"Matrix A^T is normalized: {is_normalized}\")\n",
    "        print()\n",
    "\n",
    "\n",
    "        # Coherence test\n",
    "        print(\"Coherence test\")\n",
    "        coherence = compute_coherence(Dict)\n",
    "        print(f'Coherence: {coherence}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sampling Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compressSignal(signal,Phi):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    \n",
    "    # length of signal block\n",
    "    N = Phi.shape[1]\n",
    "\n",
    "    # length of compressed block\n",
    "    M = Phi.shape[0]\n",
    "\n",
    "    # number of blocks\n",
    "    BLOCK_NUM = len(signal) // N\n",
    "\n",
    "    # each column of Y is the compressed version of a block of signal\n",
    "    Y = np.zeros((M, BLOCK_NUM))    \n",
    "    for i in range(BLOCK_NUM):\n",
    "        Y[:,i] = Phi @ signal[i*N:(i+1)*N]\n",
    "    \n",
    "    return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reconstruction method: Smooth-L0 (SL0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def SL0(y, A, sigma_min, sigma_decrease_factor=0.5, mu_0=2, L=3, A_pinv=None, showProgress=False):\n",
    "    \"\"\"\n",
    "    Returns the sparsest vector `s` that satisfies the underdetermined system of \n",
    "    linear equations `A @ s = y`, using the Smoothed L0 (SL0) algorithm.\n",
    "\n",
    "    Requires:\n",
    "    --------\n",
    "    - numpy as np\n",
    "    \n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    y : numpy array\n",
    "        The observed vector (Mx1), where M is the number of rows in `A`.\n",
    "    \n",
    "    A : numpy array\n",
    "        The measurement matrix (MxN), which should be 'wide', meaning it has more \n",
    "        columns than rows (N > M). The number of rows in `A` must match the length \n",
    "        of `y`.\n",
    "    \n",
    "    sigma_min : float\n",
    "        The minimum value of `sigma`, which determines the stopping criterion for \n",
    "        the algorithm. It should be chosen based on the noise level or desired \n",
    "        accuracy.\n",
    "    \n",
    "    sigma_decrease_factor : float, optional (default=0.5)\n",
    "        The factor by which `sigma` is decreased in each iteration. This should be \n",
    "        a positive value less than 1. Smaller values lead to quicker reduction of \n",
    "        `sigma`, possibly at the cost of accuracy for less sparse signals.\n",
    "    \n",
    "    mu_0 : float, optional (default=2)\n",
    "        The scaling factor for `mu`, where `mu = mu_0 * sigma^2`. This parameter \n",
    "        influences the convergence rate of the algorithm.\n",
    "    \n",
    "    L : int, optional (default=3)\n",
    "        The number of iterations for the inner loop (steepest descent). Increasing \n",
    "        `L` can improve the precision of the result but also increases computational \n",
    "        cost.\n",
    "    \n",
    "    A_pinv : numpy array, optional\n",
    "        The precomputed pseudoinverse of the matrix `A`. If not provided, it will be \n",
    "        calculated within the function as `np.linalg.pinv(A)`. Providing this value \n",
    "        is beneficial if the function is called repeatedly with the same `A`.\n",
    "    \n",
    "    showProgress : bool, optional (default=False)\n",
    "        If `True`, the function prints the current value of `sigma` during each \n",
    "        iteration, which helps monitor the convergence process.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    s : numpy array\n",
    "        The estimated sparse signal (Nx1) that best satisfies the equation `A @ s = y`.\n",
    "\n",
    "    Notes:\n",
    "    -----\n",
    "    - The algorithm works by iteratively reducing `sigma` in a geometric sequence, \n",
    "      starting with `sigma = 2 * max(abs(s))` and ending with `sigma_min`. At each \n",
    "      step, the function adjusts `s` to minimize the L0-norm by smoothing it using \n",
    "      a Gaussian kernel.\n",
    "    \n",
    "    - The choice of `sigma_min` is crucial: for noiseless cases, a smaller `sigma_min` \n",
    "      yields a sparser solution; for noisy cases, `sigma_min` should be a few times \n",
    "      the standard deviation of the noise in `s`.\n",
    "\n",
    "    - If `A_pinv` is precomputed and passed as an argument, the function becomes \n",
    "      more efficient, especially in scenarios where it is called repeatedly with the \n",
    "      same `A`.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if A_pinv is None:\n",
    "        A_pinv = np.linalg.pinv(A)\n",
    "\n",
    "    # Initialize the variables\n",
    "    s = A_pinv @ y\n",
    "    sigma = 2 * max(np.abs(s))\n",
    "\n",
    "    # Define lambda function for delta\n",
    "    OurDelta = lambda s, sigma: s * np.exp(-s**2 / sigma**2)\n",
    " \n",
    "    # Main loop\n",
    "    while sigma > sigma_min:\n",
    "        for i in range(L):\n",
    "            delta = OurDelta(s, sigma)\n",
    "            s = s - mu_0 * delta\n",
    "            s = s - A_pinv @ (A @ s - y)\n",
    "        \n",
    "        if showProgress:\n",
    "            print(f'sigma: {sigma}')\n",
    "\n",
    "        sigma = sigma * sigma_decrease_factor\n",
    "\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test SL0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recovery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Non-kronecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_kron_recovery(Y, sigma_min, Phi=None, Theta=None, Dict=None, \n",
    "                      basis_type='dwt', wavelet='db10', mode='per', level=None,\n",
    "                      sigma_decrease_factor=0.5, mu_0=2, L=3, Theta_pinv=None, showProgress=False):\n",
    "\n",
    "\n",
    "    ## Phi AS ARGUMENT\n",
    "    # The only purpose of giving Phi as argument is to test the same Phi for different techniques (i.e. Kronecker, Dictionaries, etc.)\n",
    "\n",
    "    # if Phi is None: both Theta and Dict should be provided\n",
    "    if Phi is None:\n",
    "        if Theta is None or Dict is None:\n",
    "            raise ValueError(\"If Phi is not provided, both Theta and Dict should be provided.\")\n",
    "    elif Theta is None and Dict is None:\n",
    "        n_block = Phi.shape[1]  # length of ORIGINAL signal block\n",
    "        Dict = generate_basis(n_block, basis_type, wavelet, mode, level) # generate dictionary\n",
    "        Theta = Phi @ Dict  # Theta\n",
    "        Theta_pinv = np.linalg.pinv(Theta)  # More-Penrose pseudoinverse of Theta\n",
    "\n",
    "    if Theta_pinv is None:\n",
    "        Theta_pinv = np.linalg.pinv(Theta)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    # print shapes and matrices\n",
    "    print(f'Y shape: {Y.shape}')\n",
    "    print(f'Theta shape: {Theta.shape}')\n",
    "    print(f'Dict shape: {Dict.shape}')\n",
    "    print(f'Theta_pinv shape: {Theta_pinv.shape}')\n",
    "    print()\n",
    "\n",
    "    print(\"Y:\")\n",
    "    printFormatted(Y)\n",
    "    print(\"Phi:\")\n",
    "    printFormatted(Phi)\n",
    "    print(\"Theta:\")\n",
    "    printFormatted(Theta)\n",
    "    print(\"Dict:\")\n",
    "    printFormatted(Dict)\n",
    "    print(\"Theta_pinv:\")\n",
    "    printFormatted(Theta_pinv)\n",
    "    print()\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "    ## IN THIS PARTICULARE CASE:\n",
    "    # Y is a matrix of size m_block x BLOCK_NUM\n",
    "    # Basically every column of Y is the compressed version of a block of signal\n",
    "\n",
    "    # I DO NOT LIKE THIS, IN A MORE REALISTIC SCENARIO THE ECG MACHINE WOULD SHURELY STORE\n",
    "    # Y AS A SINGLE ARRAY OF SIZE m_block*BLOCK_NUM, where every consecutive m_block samples\n",
    "    # are the compressed version of a block of signal\n",
    "\n",
    "    # THIS SHOULD BE CHANGED FOR BETTER REPRESENTATION\n",
    "    # Maybe we should have a function that is able to understand if Y is a matrix or a vector\n",
    "    # and act accordingly\n",
    "    BLOCK_NUM = Y.shape[1]  # number of blocks\n",
    "\n",
    "\n",
    "\n",
    "    n_block = Theta.shape[1]  # length of ORIGINAL signal block\n",
    "    x_hat = np.zeros(BLOCK_NUM*Theta.shape[1])  # recoverd signal will be long as the original signal\n",
    "    for i in range(BLOCK_NUM):\n",
    "        y = Y[:, i]\n",
    "\n",
    "        s_block = SL0(y, Theta, sigma_min, sigma_decrease_factor, mu_0, L, Theta_pinv, showProgress)\n",
    "        x_hat[i*n_block:(i+1)*n_block] = Dict @ s_block\n",
    "    \n",
    "    return x_hat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kronecker-based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kron_recovery(Y, sigma_min, Phi, kron_factor,\n",
    "                  basis_type='dwt', wavelet='db10', mode='per', level=None,\n",
    "                  sigma_decrease_factor=0.5, mu_0=2, L=3, Theta_pinv=None, showProgress=False):\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ## KRONECKER MEASUREMENT MATRIX (for reconstruction purpose only, REQUIRED)\n",
    "    # generate the measurement matrix\n",
    "    Phi_kron = np.kron(np.eye(kron_factor), Phi)\n",
    "\n",
    "\n",
    "    n_block = Phi.shape[1]  # length of ORIGINAL signal block\n",
    "    m_block = Phi.shape[0]  # length of compressed signal block\n",
    "    n_block_kron = Phi_kron.shape[1]  # length of ORIGINAL signal KRONECKER-block\n",
    "    m_block_kron = Phi_kron.shape[0]  # length of compressed signal KRONECKER-block\n",
    "\n",
    "\n",
    "    ## IN THIS PARTICULARE CASE:\n",
    "    # Y is a matrix of size m_block x BLOCK_NUM\n",
    "    # Basically every column of Y is the compressed version of a block of signal\n",
    "    # I DO NOT LIKE THIS, IN A MORE REALISTIC SCENARIO THE ECG MACHINE WOULD SHURELY STORE\n",
    "    # Y AS A SINGLE ARRAY OF SIZE m_block*BLOCK_NUM, where every consecutive m_block samples\n",
    "    # are the compressed version of a block of signal\n",
    "    # THIS SHOULD BE CHANGED FOR BETTER REPRESENTATION\n",
    "    # Maybe we should have a function that is able to understand if Y is a matrix or a vector\n",
    "    # and act accordingly\n",
    "    BLOCK_NUM = Y.shape[1]  # number of blocks\n",
    "    \n",
    "    N = n_block * BLOCK_NUM  # length of ORIGINAL signal\n",
    "    KRON_BLOCK_NUM = N // n_block_kron  # number of KRONECKER blocks\n",
    "\n",
    "\n",
    "\n",
    "    Dict_kron = generate_basis(n_block_kron, basis_type, wavelet, mode, level)  # generate dictionary (kron)\n",
    "    Theta_kron = Phi_kron @ Dict_kron  # Theta (kron)\n",
    "    Theta_kron_pinv = np.linalg.pinv(Theta_kron)  # More-Penrose pseudoinverse of Theta (kron)\n",
    "\n",
    "\n",
    "    # print shapes and matrices\n",
    "    print(f'Y shape: {Y.shape}')\n",
    "    print(f'Phi shape: {Phi.shape}')\n",
    "    print(f'Phi_kron shape: {Phi_kron.shape}')\n",
    "    print(f'Theta_kron shape: {Theta_kron.shape}')\n",
    "    print(f'Dict shape: {Dict.shape}')\n",
    "    print(f'Dict_kron shape: {Dict_kron.shape}')\n",
    "    print(f'Theta_pinv shape: {Theta_pinv.shape}')\n",
    "    print(f'Theta_kron_pinv shape: {Theta_kron_pinv.shape}')\n",
    "    print()\n",
    "\n",
    "\n",
    "\n",
    "    x_hat_kron = np.zeros(N)  # recoverd signal will be long as the original signal\n",
    "    for i in range(KRON_BLOCK_NUM):\n",
    "        y_kron = Y[:, i*kron_factor:(i+1)*kron_factor].flatten()\n",
    "\n",
    "        s_kron_block = SL0(y_kron, Theta_kron, sigma_min, sigma_decrease_factor, mu_0, L, Theta_kron_pinv, showProgress)\n",
    "        x_hat_kron[n_block_kron*i:(i+1)*n_block_kron] = Dict_kron @ s_kron_block\n",
    "\n",
    "    return x_hat_kron\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation of results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_snr(signal, recovered_signal):\n",
    "    \"\"\"\n",
    "    Calculates the Signal-to-Noise Ratio (SNR) between the original signal and the recovered signal.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    signal : numpy array\n",
    "        The original signal.\n",
    "    recovered_signal : numpy array\n",
    "        The recovered signal after some processing or recovery algorithm.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    snr : float\n",
    "        The Signal-to-Noise Ratio (SNR) in decibels (dB).\n",
    "\n",
    "    Notes:\n",
    "    -----\n",
    "    - The SNR is calculated as 20 * log10(norm(original_signal) / norm(original_signal - recovered_signal)).\n",
    "    - A higher SNR value indicates a better recovery, with less error relative to the original signal.\n",
    "    \"\"\"\n",
    "    error = recovered_signal - signal\n",
    "    snr = 20 * np.log10(np.linalg.norm(signal) / np.linalg.norm(error))\n",
    "    \n",
    "    return snr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot recovered over original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_signals(original_signal, reconstructed_signal, original_name=\"Original Signal\", reconstructed_name=\"Reconstructed Signal\"):\n",
    "    \"\"\"\n",
    "    Plots the original signal and the reconstructed signal on the same plot with the given names.\n",
    "\n",
    "    Requires:\n",
    "    ----------\n",
    "    - matplotlib.pyplot as plt\n",
    "    - numpy as np\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    original_signal : numpy array\n",
    "        The original signal to be plotted.\n",
    "    \n",
    "    reconstructed_signal : numpy array\n",
    "        The reconstructed signal to be plotted.\n",
    "    \n",
    "    original_name : str, optional (default=\"Original Signal\")\n",
    "        The name to display for the original signal in the plot.\n",
    "    \n",
    "    reconstructed_name : str, optional (default=\"Reconstructed Signal\")\n",
    "        The name to display for the reconstructed signal in the plot.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ensure the signals have the same length\n",
    "    if len(original_signal) != len(reconstructed_signal):\n",
    "        raise ValueError(\"The original signal and the reconstructed signal must have the same length.\")\n",
    "    \n",
    "    # Create the plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(original_signal, label=original_name, color='blue', linewidth=1.5)\n",
    "    plt.plot(reconstructed_signal, label=reconstructed_name, color='red', linestyle='--', linewidth=1.5)\n",
    "    plt.title(f\"{original_name} vs {reconstructed_name}\")\n",
    "    plt.xlabel('Sample Index')\n",
    "    plt.ylabel('Amplitude')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "\n",
    "    ### PREPARE\n",
    "    # ----------------------------------------------------------------\n",
    "\n",
    "    ## DATA\n",
    "    #  upload data\n",
    "    data = scipy.io.loadmat('100m.mat')\n",
    "    # retrieve the key to a string\n",
    "    key = list(data.keys())[0]\n",
    "    # retrieve the values\n",
    "    signal = data[key][0,:]  # [0 or 1, 0:650000] s.t. first dim: (0 is MLII, 1 is V5)\n",
    "    # pick only 2048 samples, because it will be Kronecker block dimension\n",
    "    start = int(512 * 4)\n",
    "    end = int(start + 512*4)\n",
    "    signal = signal[start:end]\n",
    "    # print the shape of the signal\n",
    "    print(f'Signal shape: {signal.shape}') \n",
    "    # plot the signal\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(signal)\n",
    "    plt.title('ECG Signal')\n",
    "    plt.xlabel('Index')\n",
    "    plt.ylabel('Amplitude')\n",
    "    plt.grid(True)\n",
    "    plt.show()    \n",
    "\n",
    "\n",
    "    ## PARAMETERS\n",
    "    # compression ratio\n",
    "    CR = 1/4 \n",
    "    # non-kronecker\n",
    "    n_block = 16 # block size`\n",
    "    m_block = int(n_block * CR) # compressed block size\n",
    "    # kronecker\n",
    "    n_block_kron = 512 # kron block size\n",
    "    m_block_kron = int(n_block_kron * CR) # compressed kron block size\n",
    "    kron_factor = n_block_kron // n_block # kron factor\n",
    "\n",
    "\n",
    "\n",
    "    ### SAMPLING PHASE \n",
    "    # ----------------------------------------------------------------\n",
    "\n",
    "\n",
    "    ## MEASUREMENT MATRIX\n",
    "    # generate the measurement matrix\n",
    "    Phi = generate_DBDD_matrix(m_block, n_block)\n",
    "    #Phi = generate_random_matrix(m_block, n_block, matrix_type='gaussian')\n",
    "    #Phi = generate_random_matrix(m_block, n_block, matrix_type='scaled_binary')\n",
    "    #Phi = generate_random_matrix(m_block, n_block, matrix_type='unscaled_binary')\n",
    "\n",
    "    print(f'Phi shape: {Phi.shape}')\n",
    "    print(f'Phi:')\n",
    "    print(Phi)\n",
    "\n",
    "\n",
    "    ## COMPRESS THE SIGNAL\n",
    "    # compress the signal\n",
    "    Y = compressSignal(signal, Phi)\n",
    "\n",
    "\n",
    "\n",
    "    ### RECOVERY PHASE\n",
    "    # ----------------------------------------------------------------\n",
    "\n",
    "    # note: OPTIONAL parameters that are not provided, are computed inside the functions\n",
    "\n",
    "    ## DICTIONARY non-KRONECKER (optional)\n",
    "    # generate the dictionary\n",
    "    #Dict = generate_basis(n_block, basis_type='dwt', wavelet='db10', mode='per', level=None)\n",
    "    #Dict = generate_basis(n_block, basis_type='haar_classic')\n",
    "    #Dict = generate_basis(n_block, basis_type='dct')\n",
    "\n",
    "    ## KRONECKER DICTIONARY (optional)\n",
    "    # generate the dictionary\n",
    "    #Dict = generate_basis(n_block_kron, basis_type='dwt', wavelet='db10', mode='per', level=None)\n",
    "    #Dict = generate_basis(n_block_kron, basis_type='haar_classic')\n",
    "    #Dict = generate_basis(n_block_kron, basis_type='dct')\n",
    "\n",
    "    ## KRONECKER MEASUREMENT MATRIX (for reconstruction purpose only, REQUIRED)\n",
    "    # generate the measurement matrix\n",
    "    Phi_kron = np.kron(np.eye(kron_factor), Phi)\n",
    "\n",
    "    \n",
    "    ## THETA MATRIX (optional)\n",
    "    # compute the theta matrix\n",
    "    #Theta = Phi @ Dict\n",
    "    #Theta_kron = Phi_kron @ Dict_kron\n",
    "\n",
    "\n",
    "    ## SL0 PARAMETERS\n",
    "    sigma_off = 0.001  # offset for sigma\n",
    "    Phi_pinv = np.linalg.pinv(Phi)  # precompute the pseudoinverse of Phi\n",
    "    Phi_kron_pinv = np.linalg.pinv(Phi_kron)  # precompute the pseudoinverse of Phi_kron\n",
    "    mu_0 = 2  # scaling factor for mu\n",
    "    sigma_decrease_factor = 0.5  # factor for decreasing sigma\n",
    "    L = 3  # number of iterations for the inner loop\n",
    "    if sigma_off > 0:\n",
    "        sigma_min = sigma_off * 4  # minimum value of sigma\n",
    "    else:\n",
    "        sigma_min = 0.00001  # minimum value of sigma\n",
    "\n",
    "\n",
    "\n",
    "    ## RECOVERY non-KRONECKER\n",
    "    # recover the signal\n",
    "    recovered_signal = non_kron_recovery(Y, sigma_min, Phi, Theta=None, Dict=None, basis_type='dct',\n",
    "                                          wavelet='db10', mode='per', level=None, sigma_decrease_factor=sigma_decrease_factor,\n",
    "                                            mu_0=mu_0, L=L, Theta_pinv=None, showProgress=False)\n",
    "    \n",
    "\n",
    "    ## RECOVERY KRONECKER\n",
    "    # recover the signal\n",
    "\n",
    "    recovered_signal_kron = kron_recovery(Y, sigma_min, Phi, kron_factor, basis_type='dct',\n",
    "                                          wavelet='db10', mode='per', level=None, sigma_decrease_factor=sigma_decrease_factor,\n",
    "                                            mu_0=mu_0, L=L, Theta_pinv=None, showProgress=False)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ### EVALUATION\n",
    "    # ----------------------------------------------------------------\n",
    "\n",
    "\n",
    "    ## PLOT\n",
    "    # plot the signals\n",
    "    plot_signals(signal, recovered_signal, original_name=\"Original Signal\", reconstructed_name=\"Recovered Signal\")\n",
    "\n",
    "    ## SNR\n",
    "    # calculate the SNR\n",
    "    snr = calculate_snr(signal, recovered_signal)\n",
    "    print(f'SNR: {snr} dB')\n",
    "\n",
    "    ## PLOT KRON\n",
    "    # plot the signals\n",
    "    plot_signals(signal, recovered_signal_kron, original_name=\"Original Signal\", reconstructed_name=\"Recovered Signal (Kronecker)\")\n",
    "    \n",
    "\n",
    "    ## SNR KRON\n",
    "    # calculate the SNR\n",
    "    snr_kron = calculate_snr(signal, recovered_signal_kron)\n",
    "    print(f'SNR (Kronecker): {snr_kron} dB')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".namlVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
